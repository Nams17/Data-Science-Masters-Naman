{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86efc083-c242-4059-a2ee-58f6f73534ce",
   "metadata": {},
   "source": [
    "# Q-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28d4d4d-6cbb-4eb4-92e3-dc664ab6430f",
   "metadata": {},
   "source": [
    "### Min-Max scaling, also known as normalization, is a data preprocessing technique used to transform numerical features within a specific range. It rescales the data to a common range of values, typically between 0 and 1, based on the minimum and maximum values of the feature.\n",
    "### It helps to eliminate the influence of different scales and units of measurement among features, allowing them to be directly comparable.\n",
    "### It can improve the performance of machine learning algorithms that are sensitive to the scale of the input features, such as distance-based algorithms (e.g., K-means clustering) or algorithms that rely on gradient descent optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422331d4-74dc-409f-801e-3746e0ee474e",
   "metadata": {},
   "source": [
    "### Here's an example to illustrate the application of Min-Max scaling in Python using the sklearn library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0162215-af44-49ae-94a6-5a38974109f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create an instance of MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "scaled_data = scaler.fit_transform(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be2a693-eefd-467e-9645-8b18158720c6",
   "metadata": {},
   "source": [
    "### In this example, data represents the original numerical data that needs to be scaled. First, an instance of the MinMaxScaler class is created. Then, the fit_transform() method is applied to the data, which computes the minimum and maximum values of each feature and scales the data accordingly. The resulting scaled_data will have the same number of rows and columns as the original data, but with the values scaled between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6179f95-dba3-4e10-91b6-342447198bd3",
   "metadata": {},
   "source": [
    "# Q-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e26941-124a-44e5-8c2f-4a3ee6d24c62",
   "metadata": {},
   "source": [
    "### The Unit Vector technique, also known as normalization, is a feature scaling method that scales the values of each feature to have a unit norm. Unlike Min-Max scaling, which scales the values to a specific range, Unit Vector scaling focuses on the direction or orientation of the feature vector rather than its magnitude.\n",
    "### The formula for Unit Vector scaling is:\n",
    "### unit_vector = value / norm\n",
    "### where value is the original value of a data point and norm is the Euclidean norm (magnitude) of the feature vector.\n",
    "### The main difference between Unit Vector scaling and Min-Max scaling is that Unit Vector scaling preserves the relationship between the values of the features while adjusting their magnitudes. It ensures that all feature vectors have the same length (unit norm) and points in the same direction.\n",
    "### Unit Vector scaling is particularly useful when the magnitude of the feature values is less important compared to their relative orientations or directions. It is commonly used in text classification or document analysis tasks, where the frequency of occurrence of words in a document is more relevant than the actual counts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418f72e5-111b-4890-aa3c-fc1b19815c9a",
   "metadata": {},
   "source": [
    "### Example to illustrate the application of Unit Vector scaling in Python using the sklearn library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77091576-7e01-4fdd-ae32-56148f8ed59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# Create an instance of Normalizer\n",
    "scaler = Normalizer(norm='l2')\n",
    "\n",
    "# Fit and transform the data\n",
    "scaled_data = scaler.fit_transform(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3d7cfd-5835-4340-997b-f67a46724595",
   "metadata": {},
   "source": [
    "### In this example, data represents the original numerical data that needs to be scaled. An instance of the Normalizer class is created, specifying the normalization norm as 'l2' to calculate the Euclidean norm. Then, the fit_transform() method is applied to the data, which scales the values of each feature vector to have a unit norm. The resulting scaled_data will have the same number of rows and columns as the original data, with each feature vector having a unit norm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56187cbf-b949-4ddd-b61c-70f8113cc407",
   "metadata": {},
   "source": [
    "# Q-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515499ed-564a-402d-8b3d-cbea4d0bfea0",
   "metadata": {},
   "source": [
    "### PCA, which stands for Principal Component Analysis, is a popular dimensionality reduction technique used to transform a high-dimensional dataset into a lower-dimensional space while retaining most of the important information. It achieves this by identifying the principal components, which are linear combinations of the original features, that capture the maximum amount of variance in the data.\n",
    "### Example to illustrate the application of PCA in Python using the sklearn library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e8578a-dc62-4fc1-9bbe-5020e243f050",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create an instance of PCA with the desired number of components\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit and transform the data\n",
    "reduced_data = pca.fit_transform(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db8d995-c187-4e94-8fb1-630bdb449f6c",
   "metadata": {},
   "source": [
    "### In this example, data represents the original high-dimensional dataset. An instance of the PCA class is created with the n_components parameter set to 2, indicating that we want to retain 2 principal components. The fit_transform() method is then applied to the data, which performs PCA on the data and returns the transformed lower-dimensional representation stored in reduced_data. This reduced data can be used for visualization, analysis, or as input to downstream machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813d0438-f13f-4376-a356-9fc326d5670a",
   "metadata": {},
   "source": [
    "# Q-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474a23b2-5348-46e3-bd26-b7f8a4e0edd8",
   "metadata": {},
   "source": [
    "### PCA (Principal Component Analysis) can be used for feature extraction, which involves selecting or creating a subset of features that are most relevant or informative for a given task. Feature extraction aims to reduce the dimensionality of the dataset while preserving as much relevant information as possible.\n",
    "### The relationship between PCA and feature extraction lies in the fact that PCA can be used to transform the original features into a new set of uncorrelated variables, called principal components. These principal components are linear combinations of the original features and capture the maximum variance in the data. By selecting a subset of the top-ranked principal components, we can effectively perform feature extraction.\n",
    "### Example to illustrate how PCA can be used for feature extraction in Python using the sklearn library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2eb4653-37e5-44f5-863e-9ddb39238eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create an instance of PCA with the desired number of components\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit and transform the data\n",
    "reduced_features = pca.fit_transform(data)\n",
    "\n",
    "# Use the transformed features for further analysis or modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7861f60-6c24-41c0-95a8-a34c637d870d",
   "metadata": {},
   "source": [
    "### In this example, data represents the original dataset with multiple features. We create an instance of the PCA class with n_components set to 2, indicating that we want to extract 2 features. The fit_transform() method is then applied to the data, which performs PCA and returns the transformed features stored in reduced_features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7c941f-c080-41ff-b5a3-b3d2cf92370f",
   "metadata": {},
   "source": [
    "### The resulting reduced_features will have a reduced dimensionality compared to the original dataset, with each sample represented by two principal components. These components can be considered as the extracted features that capture the most important information in the data. We can then use these transformed features for further analysis, visualization, or as input to machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3206b65-9583-4e38-8785-d3a37542de8c",
   "metadata": {},
   "source": [
    "# Q-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451e9c27-21e3-4463-8272-46a2b00665d9",
   "metadata": {},
   "source": [
    "### In the context of building a recommendation system for a food delivery service, Min-Max scaling can be used to preprocess the numerical features like price, rating, and delivery time. The goal of Min-Max scaling is to rescale the values of these features to a common range, typically between 0 and 1, based on their minimum and maximum values.\n",
    "### Step-by-step explanation of how Min-Max scaling can be applied to preprocess the data:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313c3eeb-d622-460d-9ed3-da231a7c03aa",
   "metadata": {},
   "source": [
    "### 1. Identify the numerical features: In this case, we have features like price, rating, and delivery time, which are numerical and need to be scaled.\n",
    "### 2. Compute the minimum and maximum values: Calculate the minimum and maximum values of each feature. This can be done by iterating through the dataset or using built-in functions like min() and max().\n",
    "### 3. Apply Min-Max scaling formula.\n",
    "### 4. Perform Min-Max scaling: Use the computed minimum and maximum values to scale the feature values accordingly. This can be done manually or using libraries like sklearn.preprocessing.MinMaxScaler in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9547720-0829-4d90-86b2-61b47257845f",
   "metadata": {},
   "source": [
    "### By applying Min-Max scaling, the numerical features will be transformed to a common range between 0 and 1. This ensures that the features contribute equally to the recommendation system and avoids any bias caused by differences in their scales. It also allows for direct comparison and similarity calculations among the features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306fc08d-f2d1-4469-a463-8697585e4f9a",
   "metadata": {},
   "source": [
    "### Once the data is preprocessed using Min-Max scaling, it can be used as input to train recommendation models, such as collaborative filtering or content-based filtering, to generate personalized recommendations for users based on their preferences, price ranges, ratings, and delivery times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fe9846-114e-489a-b955-0b77f734a430",
   "metadata": {},
   "source": [
    "# Q-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f871ecf-a222-4c41-9ca8-dcf18457dfba",
   "metadata": {},
   "source": [
    "### In the context of building a model to predict stock prices with a dataset containing multiple features, PCA (Principal Component Analysis) can be used to reduce the dimensionality of the dataset. The goal of PCA is to transform the original features into a new set of uncorrelated variables, called principal components, while retaining most of the important information.\n",
    "### An overview of how PCA can be applied to reduce the dimensionality of the dataset:\n",
    "### 1.Preprocess the data: Start by preprocessing the dataset, which may involve steps like handling missing values, normalizing or standardizing the features, and encoding categorical variables if applicable.\n",
    "### 2.Standardize the features: PCA assumes that the features are standardized with zero mean and unit variance. Therefore, it's important to standardize the features to ensure they contribute equally to the analysis.\n",
    "### 3.Apply PCA: Once the data is preprocessed, apply PCA to the standardized features. PCA computes the principal components, which are linear combinations of the original features, capturing the maximum variance in the data.\n",
    "### 4.Determine the number of components: Evaluate the cumulative explained variance ratio or eigenvalues associated with each principal component to determine the number of components to retain. Typically, you aim to retain a sufficient number of components that explain a significant portion of the variance (e.g., 80-90% or higher).\n",
    "### 5.Project the data onto the selected components: After determining the desired number of components, project the standardized features onto those components to obtain the reduced-dimensional representation of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228d12d0-4c9e-4532-9206-9b1ca0754fdb",
   "metadata": {},
   "source": [
    "### By reducing the dimensionality of the dataset using PCA, several benefits can be achieved:\n",
    "\n",
    "### Dimensionality reduction: PCA helps to reduce the number of features, which can improve computational efficiency, reduce noise, and address the curse of dimensionality.\n",
    "\n",
    "### Eliminating multicollinearity: PCA transforms the original features into uncorrelated principal components, addressing issues of multicollinearity that may exist in the dataset.\n",
    "\n",
    "### Capturing important information: PCA retains the most important information by selecting the principal components that explain the most variance in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57456255-035c-4b7c-84de-08b2da0b5e49",
   "metadata": {},
   "source": [
    "### After applying PCA, the reduced-dimensional dataset can be used as input for training a stock price prediction model, such as regression or time series forecasting models. The reduced features can provide a more compact representation of the original data while still capturing the essential patterns and relationships necessary for accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9591d15-e780-4425-bd6c-5c7bee51f528",
   "metadata": {},
   "source": [
    "# Q-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9494ec1f-ec42-41fd-9669-f90ff2ebe84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "values = [1, 5, 10, 15, 20]\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "\n",
    "values_2d = [[value] for value in values]\n",
    "\n",
    "scaled_data = scaler.fit_transform(values_2d)\n",
    "\n",
    "scaled_flat_data = [[value[0]] for value in scaled_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3b12fff-f3c9-433f-98fe-5d57be430863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-0.9999999999999999],\n",
       " [-0.5789473684210525],\n",
       " [-0.05263157894736836],\n",
       " [0.47368421052631593],\n",
       " [1.0]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_flat_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6562ad5-0153-47a0-b2a4-f9439fb1c369",
   "metadata": {},
   "source": [
    "# Q-8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf8f009-5db1-4699-b23d-16450d1df936",
   "metadata": {},
   "source": [
    "### To determine the number of principal components to retain for feature extraction using PCA, you typically consider the cumulative explained variance ratio or eigenvalues associated with each principal component. The cumulative explained variance ratio indicates the amount of variance explained by each principal component in relation to the total variance in the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
