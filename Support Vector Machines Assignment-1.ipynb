{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3ed61a9-be31-498a-a042-67709c657f00",
   "metadata": {},
   "source": [
    "# Q-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb886415-688a-419c-bdeb-6d6ae5534f65",
   "metadata": {},
   "source": [
    "### A linear Support Vector Machine (SVM) is used for binary classification and can be represented by the following mathematical formula:\n",
    "### For a given feature vector x, the decision function for a linear SVM is defined as:\n",
    "f(x)=sign(w⋅x+b)\n",
    "### Where:\n",
    "- �\n",
    "(\n",
    "�\n",
    ")\n",
    "f(x) is the decision function.\n",
    "- �\n",
    "w is the weight vector.\n",
    "- x is the feature vector.\n",
    "- b is the bias term.\n",
    "- ⋅ represents the dot product between \n",
    "�\n",
    "w and \n",
    "�\n",
    "x.\n",
    "- sign(⋅) is the sign function, which returns \n",
    "+\n",
    "1\n",
    "+1 if its argument is positive or zero and \n",
    "−\n",
    "1\n",
    "−1 if its argument is negative.\n",
    "\n",
    "The goal of training a linear SVM is to find the values of \n",
    "�\n",
    "w and \n",
    "�\n",
    "b that maximize the margin between the two classes while minimizing classification errors. This optimization problem can be formally represented as a quadratic programming problem:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37055c6b-9fb7-4a74-b637-f98df55f0c65",
   "metadata": {},
   "source": [
    "# Q-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3191cca6-43f3-4d2b-a55b-349c1680a22f",
   "metadata": {},
   "source": [
    "### The objective function of a linear Support Vector Machine (SVM) is to find the parameters (weight vector �w and bias term �b) that maximize the margin between two classes while minimizing classification errors. This objective function can be expressed as an optimization problem. For a linearly separable dataset, the objective function for a soft-margin SVM can be defined as:\n",
    "\n",
    "Minimize:  \n",
    "2\n",
    "1\n",
    "​\n",
    " ∥w∥ \n",
    "2\n",
    " +C \n",
    "i=1\n",
    "∑\n",
    "N\n",
    "​\n",
    " ξ \n",
    "i\n",
    "​\n",
    "\n",
    "### Subject to the constraints:\n",
    "y \n",
    "i\n",
    "​\n",
    " (w⋅x \n",
    "i\n",
    "​\n",
    " +b)≥1−ξ \n",
    "i\n",
    "​\n",
    " for all i=1,2,…,N\n",
    " \n",
    "### Where:\n",
    "- w is the weight vector.\n",
    "- �b is the bias term.\n",
    "- C is a regularization parameter that controls the trade-off between maximizing the margin and minimizing classification errors. A smaller �C results in a larger margin but may allow some misclassifications, while a larger �C penalizes misclassifications more heavily.\n",
    "- N is the number of training samples.\n",
    "- ��ξ i​\n",
    "  is a non-negative slack variable associated with the �i-th training sample. It represents the distance by which the �i-th sample falls on the wrong side of the margin.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7df8f4-dcce-4596-a550-34d8382c199b",
   "metadata": {},
   "source": [
    "# Q-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06065927-68c5-418e-8629-969c60395638",
   "metadata": {},
   "source": [
    "### The kernel trick is a fundamental concept in Support Vector Machines (SVMs) that allows them to handle non-linearly separable datasets and perform non-linear classification. It's a technique that transforms the input data into a higher-dimensional space where it becomes linearly separable without explicitly computing the transformation. This is done by using a mathematical function called a kernel function, which calculates the inner product between the transformed feature vectors in the higher-dimensional space.\n",
    "### The key idea behind the kernel trick is to map the original feature space into a higher-dimensional space, where the classes become linearly separable. However, rather than explicitly mapping the data to this higher-dimensional space, the kernel function computes the similarity (dot product) between the mapped data points without having to calculate the transformation explicitly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa2e122-7961-414a-8005-caf194c2b7e7",
   "metadata": {},
   "source": [
    "# Q-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0968b9-5065-48d6-bdd8-49aa998221d5",
   "metadata": {},
   "source": [
    "### Support vectors are critical elements in Support Vector Machines (SVMs) and play a fundamental role in determining the decision boundary and the overall performance of the SVM classifier. Let's explore the role of support vectors with an example:\n",
    "### Example: Binary Classification\n",
    "### Suppose you have a binary classification problem where you want to distinguish between two classes, A and B, based on a set of features. Your data looks like this:\n",
    "\n",
    "Class A (represented by red dots)\n",
    "\n",
    "Feature 1: [1, 2, 2.5, 3, 4]\n",
    "Feature 2: [2, 1, 3, 4, 3]\n",
    "\n",
    "Class B (represented by blue dots)\n",
    "\n",
    "Feature 1: [7, 7.5, 8, 9, 9.5]\n",
    "Feature 2: [5, 6, 6.5, 7, 8]\n",
    "\n",
    "### Now, you want to train an SVM to separate these two classes.\n",
    "### Role of Support Vectors:\n",
    "### Identifying Support Vectors: When you train an SVM, it tries to find the hyperplane (decision boundary) that maximizes the margin between the two classes while minimizing classification errors. Support vectors are data points from both classes that are closest to this margin or hyperplane. In other words, they are the points that \"support\" the margin.\n",
    "### Defining the Margin: The margin is the region between the two parallel hyperplanes that are equidistant from the decision boundary (hyperplane). The support vectors are the data points that lie on or within the margin. These are the most critical data points for determining the position and orientation of the decision boundary.\n",
    "### Determining the Decision Boundary: The decision boundary is primarily defined by the support vectors. In a binary classification problem like this, the decision boundary is a hyperplane that aims to maximize the distance between the nearest support vectors of each class. It is placed equidistant from the support vectors of both classes.\n",
    "### Robustness and Generalization: Support vectors are crucial for the robustness and generalization of the SVM model. They represent the data points that are most challenging to classify correctly. By focusing on these points, SVMs ensure that the model can generalize well to unseen data.\n",
    "### Sparsity: SVMs are known for their sparsity, which means that only a small subset of the training data (the support vectors) contributes to defining the decision boundary. This sparsity property makes SVMs efficient in high-dimensional spaces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113212a3-561e-40e6-9e03-cf22bfca13fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
